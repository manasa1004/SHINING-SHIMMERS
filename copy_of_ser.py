# -*- coding: utf-8 -*-
"""Copy of SER.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A5vNDM3r3L-DqzHn09jfEtR_ifSngJiG
"""
#Mounting to google drive
from google.colab import drive
drive.mount('/content/drive')

#Import Statements
import librosa
import soundfile
import os, glob,pickle
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score

#Feature Extraction
def extract_feature(file_name, mfcc, chroma, mel):
    with soundfile.SoundFile(file_name) as sound_file:
        X = sound_file.read(dtype="float32")
        sample_rate=sound_file.samplerate
        if chroma:
            stft=np.abs(librosa.stft(X))
        result=np.array([])
        if mfcc:
            mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate,
n_mfcc=40).T, axis=0)
            result=np.hstack((result, mfccs))
            if chroma:
                chroma=np.mean(librosa.feature.chroma_stft(S=stft,
sr=sample_rate).T,axis=0)
                result=np.hstack((result, chroma))
            if mel:
                mel=np.mean(librosa.feature.melspectrogram(X,
sr=sample_rate).T,axis=0)
                result=np.hstack((result, mel))
    return result

#Total Emotions
emotions={'01':'neutral','02':'calm','03':'happy','04':'sad','05':'angry',
'06':'fearful','07':'disgust','08':'surprised'}
print("Emotions in the data set are : " , emotions)

#Observed Emotions
observed_emotions=['angry', 'happy', 'neutral', 'sad']
print("Emotions being observed are : " , observed_emotions)

#Loading the data and extract features for each sound file
def load_data(test_size=0.25):
  x,y=[],[]
  for file in glob.glob("/content/drive/MyDrive/DATASET/Dataset/RAVDESS_Dataset/Actor_*/*.wav"):
    file_name=os.path.basename(file)
    emotion=emotions[file_name.split("-")[2]]
    if emotion not in observed_emotions:
      continue
    try:
      feature=extract_feature(file, mfcc=True, chroma=True, mel=True)
    except:
      continue
    x.append(feature)
    y.append(emotion)
  return train_test_split(np.array(x), y, test_size=test_size, random_state=9)

#Split the dataset
x_train,x_test,y_train,y_test=load_data(test_size=0.25)
print((x_train.shape[0], x_test.shape[0]))

#Creating numpy files
np.save('x_train.npy', x_train) # save
np.load('x_train.npy') # load

np.save('x_test.npy', x_test) 
np.load('x_test.npy') 

np.save('y_train.npy', y_train) 
np.load('y_train.npy') 

np.save('y_test.npy', y_test)
np.load('y_test.npy')

#Get the number of features extracted
print(f'Features extracted: {x_train.shape[1]}')

#Initialise the Multi Layer Perceptron Classifier
model=MLPClassifier(alpha=0.01, batch_size=256, epsilon=1e-08,
hidden_layer_sizes=(300,), learning_rate='adaptive', max_iter=500)

#Train the model
model.fit(x_train,y_train)

#Predict for the test set
y_pred=model.predict(x_test)

#Calculate the accuracy of model
accuracy=accuracy_score(y_true=y_test, y_pred=y_pred)

#Print the accuracy
print("Accuracy: {:.2f}%".format(accuracy*100))

#serialization
import joblib
joblib.dump(model,'speech-emotion')

#deserialization
import joblib
text_model=joblib.load('speech-emotion')

pip install streamlit --quiet

import streamlit as st

import joblib
import streamlit as st
import wave
from scipy.io import wavfile


uploaded_file = st.file_uploader("Choose a file")
#uploaded_file = wavfile.read('/content/drive/MyDrive/DATASET/Dataset/RAVDESS_Dataset/Actor_01/03-01-01-01-01-01-01.wav')
if uploaded_file is not None:
  # To read file as bytes:
    bytes_data = uploaded_file.getvalue(uploaded_file)
    st.write(bytes_data)
dataframe = wave.open(bytes_data,mode="rb")
st.write(dataframe)
st.audio(data, format="audio/wav", start_time=0)
text_model = joblib.load('/content/speech-emotion')
op = text_model.predict(dataframe)

if st.button('PREDICT'):
   st.title("Entered Emotion:",op)

!streamlit run app.py &npx localtunnel --port 8501
