# -*- coding: utf-8 -*-
"""Copy of SER.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A5vNDM3r3L-DqzHn09jfEtR_ifSngJiG
"""

from google.colab import drive
drive.mount('/content/drive')

import librosa
import soundfile
import os, glob,pickle
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score

def extract_feature(file_name, mfcc, chroma, mel):
    with soundfile.SoundFile(file_name) as sound_file:
        X = sound_file.read(dtype="float32")
        sample_rate=sound_file.samplerate
        if chroma:
            stft=np.abs(librosa.stft(X))
        result=np.array([])
        if mfcc:
            mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate,
n_mfcc=40).T, axis=0)
            result=np.hstack((result, mfccs))
            if chroma:
                chroma=np.mean(librosa.feature.chroma_stft(S=stft,
sr=sample_rate).T,axis=0)
                result=np.hstack((result, chroma))
            if mel:
                mel=np.mean(librosa.feature.melspectrogram(X,
sr=sample_rate).T,axis=0)
                result=np.hstack((result, mel))
    return result

emotions={'01':'neutral','02':'calm','03':'happy','04':'sad','05':'angry',
'06':'fearful','07':'disgust','08':'surprised'}
print("Emotions in the data set are : " , emotions)

observed_emotions=['angry', 'happy', 'neutral', 'sad']
print("Emotions being observed are : " , observed_emotions)

def load_data(test_size=0.25):
  x,y=[],[]
  for file in glob.glob("/content/drive/MyDrive/DATASET/Dataset/RAVDESS_Dataset/Actor_*/*.wav"):
    file_name=os.path.basename(file)
    emotion=emotions[file_name.split("-")[2]]
    if emotion not in observed_emotions:
      continue
    try:
      feature=extract_feature(file, mfcc=True, chroma=True, mel=True)
    except:
      continue
    x.append(feature)
    y.append(emotion)
  return train_test_split(np.array(x), y, test_size=test_size, random_state=9)

!touch /content/utils.py

x_train,x_test,y_train,y_test=load_data(test_size=0.25)
print((x_train.shape[0], x_test.shape[0]))

np.save('x_train.npy', x_train) # save
np.load('x_train.npy') # load

np.save('x_test.npy', x_test) # save
np.load('x_test.npy') # load

np.save('y_train.npy', y_train) # save
np.load('y_train.npy') # load

np.save('y_test.npy', y_test) # save
np.load('y_test.npy') # load

print(f'Features extracted: {x_train.shape[1]}')

model=MLPClassifier(alpha=0.01, batch_size=256, epsilon=1e-08,
hidden_layer_sizes=(300,), learning_rate='adaptive', max_iter=500)

model.fit(x_train,y_train)

y_pred=model.predict(x_test)

accuracy=accuracy_score(y_true=y_test, y_pred=y_pred)

print("Accuracy: {:.2f}%".format(accuracy*100))

#serialization
import joblib
joblib.dump(model,'speech-emotion')

#deserialization
import joblib
text_model=joblib.load('speech-emotion')

pip install streamlit --quiet

import streamlit as st

# Commented out IPython magic to ensure Python compatibility.
# %%writefile demo.py
# import streamlit as st
# 
# st.title("hello")
# st.write("my web app")
#

!streamlit run demo.py &npx localtunnel --port 8501

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import joblib
# 
# st.title("Speech Emotion Recognition Using ML")
# uploaded_file = st.file_uploader("Choose a file")
# if uploaded_file is not None:
#      # To read file as bytes:
#      bytes_data = uploaded_file.getvalue()
#      st.write(bytes_data)
# 
# 
#      # To read file as string:
#      string_data = stringio.read()
#      st.write(string_data)
# 
#      # Can be used wherever a "file-like" object is accepted:
#      dataframe = pd.read_wav(uploaded_file)
#      st.write(dataframe)
# 
# 
# 
#

import joblib
import streamlit as st
import wave
from scipy.io import wavfile


uploaded_file = st.file_uploader("Choose a file")
#uploaded_file = wavfile.read('/content/drive/MyDrive/DATASET/Dataset/RAVDESS_Dataset/Actor_01/03-01-01-01-01-01-01.wav')
if uploaded_file is not None:
  # To read file as bytes:
    bytes_data = uploaded_file.getvalue(uploaded_file)
    st.write(bytes_data)
dataframe = wave.open(bytes_data,mode="rb")
st.write(dataframe)
st.audio(data, format="audio/wav", start_time=0)
text_model = joblib.load('/content/speech-emotion')
op = text_model.predict(dataframe)


if st.button('PREDICT'):
   st.title("Entered Emotion:",op)

!streamlit run app.py &npx localtunnel --port 8501

pip install pydub

# %%writefile app.py

from pydub import AudioSegment
import streamlit as st
import librosa
import soundfile
import os, glob, pickle
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score

import streamlit as st

st.title("Speech Emotion Recognition System Using ML")
file_upload = st.file_uploader("Insert file")

st.file_uploader?

print(file_upload)

#Extracting Features of speech audio files
def extract_feature(file_name, mfcc, chroma, mel):
  X = file_name.read(dtype="float32")
  sample_rate=file_name.samplerate
  if chroma:
    stft=np.abs(librosa.stft(X))
    result=np.array([])
  if mfcc:
    mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate,n_mfcc=40).T, axis=0)
    result=np.hstack((result, mfccs))
  if chroma:
    chroma=np.mean(librosa.feature.chroma_stft(S=stft,sr=sample_rate).T,axis=0)
    result=np.hstack((result, chroma))
  if mel:
    mel=np.mean(librosa.feature.melspectrogram(X,sr=sample_rate).T,axis=0)
    result=np.hstack((result, mel))
  return result

#Emotions in the dataset
emotions={'01':'neutral','02':'calm','03':'happy','04':'sad','05':'angry','06':'fearful','07':'disgust','08':'surprised'}

#Observing emotions
observed_emotions=['angry', 'happy', 'neutral', 'sad']

#Load the info data and extract features for every sound file
def load_data(name):
  t = []
  x = AudioSegment.from_file("/content/drive/MyDrive/DATASET/Dataset/RAVDESS_Dataset/Actor_01/" +name, format = "wav")
  audio = open("/content/drive/MyDrive/DATASET/Dataset/RAVDESS_Dataset/Actor_01/" + name, 'rb')
  st.audio(audio, format='wav')
  mono_audios = x.split_to_mono()
  mono = mono_audios[0].export("/content/drive/MyDrive/DATASET/Dataset/RAVDESS_Dataset/Actor_01/03-01-01-01-01-01-01.wav",format="wav")


  h = soundfile.SoundFile(mono)

  feature=extract_feature(h, mfcc=True, chroma=True, mel=True)
  t.append(feature)
  return t

import numpy as np
x_train = np.load("/content/x_train.npy")
x_test  = np.load("/content/x_test.npy")
y_train = np.load("/content/y_train.npy")
y_test  = np.load("/content/y_test.npy")

if file_upload:
  x = load_data(file_upload.name)
  print(x)
  x = np.array(x)

#find shape of the training , testing datasets
print((x_train.shape[0], x_test.shape[0]))

#to find the number of features extracted
print(f'Features extracted: {x_train.shape[1]}')

#Initializing the MLP(Multi layer preceptron) Classifier
model = MLPClassifier(alpha=0.01, batch_size=256, epsilon=1e-08,hidden_layer_sizes=(300,), learning_rate='adaptive', max_iter=500)


model.fit(x_train,y_train)

#Prediction of test set
y_pred=model.predict(x_test)

#accuracy calculation of our model
accuracy=accuracy_score(y_true=y_test, y_pred=y_pred)

out = model.predict(x.reshape(1, -1))
st.subheader("Emotion : " + out[0])

!streamlit run app.py &npx localtunnel --port 8501