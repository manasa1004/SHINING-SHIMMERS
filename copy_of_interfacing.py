# -*- coding: utf-8 -*-
"""Copy of Interfacing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bzZZ9NnElBm4SIc4haxTj77fiMtv4u4k
"""



from google.colab import drive
drive.mount('/content/drive')





pip install streamlit

#import streamlit as st

!pip install pyngrok

!pip install pydub

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# from pydub import AudioSegment
# import streamlit as st
# import librosa
# import soundfile
# import os, glob, pickle
# import numpy as np
# from sklearn.model_selection import train_test_split
# from sklearn.neural_network import MLPClassifier
# from sklearn.metrics import accuracy_score
# st.title("Speech Emotion Recognition System Using ML")
# file_upload = st.file_uploader("Insert file")
# def extract_feature(file_name, mfcc, chroma, mel):
#   X = file_name.read(dtype="float32")
#   sample_rate=file_name.samplerate
#   if chroma:
#     stft=np.abs(librosa.stft(X))
#   result=np.array([])
#   if mfcc:
#     mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate,n_mfcc=40).T, axis=0)
#     result=np.hstack((result, mfccs))
#   if chroma:
#     chroma=np.mean(librosa.feature.chroma_stft(S=stft,sr=sample_rate).T,axis=0)
#     result=np.hstack((result, chroma))
#   if mel:
#     mel=np.mean(librosa.feature.melspectrogram(X,sr=sample_rate).T,axis=0)
#     result=np.hstack((result, mel))
#   return result
# 
# emotions={'01':'neutral','02':'calm','03':'happy','04':'sad','05':'angry','06':'fearful','07':'disgust','08':'surprised'}
# observed_emotions=['angry', 'happy', 'neutral', 'sad']
# 
# def load_data(name):
#   t = []
#   x = AudioSegment.from_file("/content/drive/MyDrive/WISE - ML Project(SER)" +name, format = "wav")
#   audio = open("/content/drive/MyDrive/WISE - ML Project(SER)" + name, 'rb')
#   st.audio(audio, format='wav')
#   mono_audios = x.split_to_mono()
#   mono = mono_audios[0].export("/content/drive/MyDrive/WISE - ML Project(SER)/test1.wav",format="wav")
#   h = soundfile.SoundFile(mono)
# 
#   feature=extract_feature(h, mfcc=True, chroma=True, mel=True)
#   t.append(feature)
#   return t
# 
# x_train = np.load("drive/MyDrive/x_train.npy")
# x_test = np.load("drive/MyDrive/x_test.npy")
# y_train = np.load("drive/MyDrive/y_train.npy")
# y_test = np.load("drive/MyDrive/y_test.npy")
# 
# if file_upload:
#   x = load_data(file_upload.name)
#   print(x)
#   x = np.array(x)
#   print((x_train.shape[0], x_test.shape[0]))
#   print(f'Features extracted: {x_train.shape[1]}')
#   model = MLPClassifier(alpha=0.01, batch_size=256, epsilon=1e-08,hidden_layer_sizes=(300,), learning_rate='adaptive', max_iter=500)
#   model.fit(x_train,y_train)
#   y_pred=model.predict(x_test)
#   accuracy=accuracy_score(y_true=y_test, y_pred=y_pred)
#   out = model.predict(x.reshape(1, -1))
#   st.subheader("Emotion : " + out[0])

!ngrok authtoken 28Bxkhmd7Ag1jgYvg7zbxLX84er_5VKneUmFkaJgAQRgKqVej

from pyngrok import ngrok
!nohup streamlit run app.py &
url = ngrok.connect(port = 8501)
print(url)

!streamlit run --server.port 80 app.py >/dev/null

